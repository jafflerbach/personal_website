---
title: "Santa Barbara's nature-based 'Sense of Place', as told by twitter"
author: Jamie Montgomery
date: '2020-03-31'
slug: santa-barbara-twitter
categories: []
tags: []
---

```{r setup, echo=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, fig.align = "center")

library(mapview)
library(tidyverse)
library(sf)
library(leaflet)
library(kableExtra)
library(sp)
library(ggpol) #for the facet_share function
library(RColorBrewer)
library(ggrepel)
library(patchwork)
library(ggmap)
library(ggtext)


tweet_data <- read_csv("~/github/sb_sense_of_place/data/geotagged_sb_tweets_post_apr_2015.csv")
mapviewOptions(basemaps = c("CartoDB.Positron", "OpenStreetMap"))

register_google(Sys.getenv("GOOGLE_ACCESS_TOKEN"))
```

# Summary

**The aim of this project was to evaluate whether or not geotagged social media data can be useful in providing insight into a region's "Sense of Place" using Santa Barbara as a case study.** 

Sense of Place can be defined as the connection people feel to their geographic surroundings, including both the natural and built environment. Locations with a strong sense of place often have a strong identity felt by both locals and visitors. 

# Findings

Not surprisingly, tourists and locals both tweet about nature. Tourists tweet about nature more (X%), but stick to the popular tourist sites in town including the wharf, waterfront, zoo, santa barbara bowl and more. Santa barbara locals are also found at these sites just not as in high a proportion. Natural areas that are further from the downtown

There is significant overlap in tourist and local patterns within the downtown area, indicating that tourists and locals alike share a fondness for the same areas and things. 

# Geotagged social media data in conservation

Geotagged social media data has been used in recent years to study people's interaction with the natural environment in various ways, many of which are focused on tourism:  

- Quantifying nature-based tourism (Wood et al. 2013, Kim et al. 2019)  
- Mapping tourist footprints (Runge & Daigle 2020), flows (Chua et al. 2016), and hot spots (Garcia-Palomares et al. 2015)  
- Understand tourist preferences in nature based places such as Kruger National Park (Hausmann et al. 2017, Levin et al. 2017, Tenkanen  et al. 2017)  
- Monitor and measure environmental conditions of places (e.g. Great Barrier Reef, Becken et al. 2017)  

This project differs in that I wanted to map the spatial patterns of tourists and locals, and understand how these two user groups engage with and perceive the natural environment of Santa Barbara. 

It also gave me a chance to learn new text mining tools.

# Why Santa Barbara?

The easy answer - I live here! Since I know the city and surrounding areas rather well, I could quickly look at spatial patterns and understand what exists at that location. The total number of tweets coming from Santa Barbara is also manageable compared to a much larger urban city.

Also, Santa Barbara is known for being a tourist town, and having beautiful natural and built landscapes (ok - I might be a bit biased here). Santa Barbara sits between the mountains and the ocean just 1.5 hours north of LA and has excellent recreation, dining, entertainment options. It's no surprise that a lot of UCSB students end up sticking around after graduation, myself included `r emo::ji('raising hand')`.

# Finding the data

Going into this project, I thought that twitter data would be easily accessibly based on the number of different projects I had been seeing that used Twitter data and related R packages. But I quickly learned that this was not the case and Twitter only allows free public access to past 9 days of tweets. This was a problem since we wanted all tweets from January 1, 2015 - December 31, 2019.

Twitter data was obtained freely through an established partnership between UCSB Library and Crimson Hexagon. Before downloading, the data was queried to meet the following conditions:

1. Tweet came from the Santa Barbara area
2. Only original tweets (no retweets)
3. Date was marked between January 1, 2015 and December 31, 2019

Crimson Hexagon only allows 10,000 randomly selected tweets to be exported, manually, at a time in .xls format. Due to this restriction, data was manually downloaded for every 2 days in order to capture all tweets (`r emo::ji('sweat')`). This took a significant amount of point and click time as you can imagine!

Once downloaded, the twitter data did not contain all desired information, including whether or not the tweet was geotagged which was vital to this project. To get this information I stepped outside of my R comfort zone and used the python `twarc` library. This library can  be used to "rehydrate" twitter data using individual tweet ids, and  then store all associated tweet information as .json files. From here I was able to remove all tweets that did not have a geotag, giving a total of **79,981** tweets.


## Twitter data

Here is a sample of the tweet data:

```{r sample_tweet_table}
kable(sample_n(tweet_data %>% select(-user_id,  -geo_coordinates, -lat, -lon), 7)) %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), font_size = 11, fixed_thead = T)
```

Trying basic leaflet map

```{r out.width = '100%'}
dc_crime <- sf::st_read("https://opendata.arcgis.com/datasets/dc3289eab3d2400ea49c154863312434_8.geojson", quiet = TRUE)

leaflet() %>% 
  addProviderTiles(providers$CartoDB.Positron)%>% 
  addCircles(data = dc_crime, color = "#800000")
```
